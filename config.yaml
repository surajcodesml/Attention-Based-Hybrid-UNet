# Configuration file for Attention-Based Hybrid U-Net OCT Layer Segmentation
# Optimized for NVIDIA A100 80GB GPU
# For GTX 1650 Ti 4GB: See comments with "GTX 1650 Ti" for recommended values

# Dataset configuration
dataset:
  hdf5_path: "data/Nemours_Jing_0805.h5"
  original_size:
    height: 496
    width: 768
  target_size:
    height: 480
    width: 768
  num_classes: 3  # 0: above ILM, 1: ILM to BM, 2: below BM

# Preprocessing configuration
preprocessing:
  gaussian_blur:
    enabled: true
    kernel_size: 5
    sigma: 1.0
  clahe:
    enabled: true
    clip_limit: 2.0
    tile_grid_size: [8, 8]
  normalization:
    mean: 0.0
    std: 1.0
    # Image values will be normalized to [0, 1] then standardized

# Data augmentation configuration
augmentation:
  # Geometric transformations (applied to both images and masks)
  geometric:
    horizontal_flip:
      enabled: true
      probability: 0.5
    vertical_flip:
      enabled: true
      probability: 0.5
    rotation:
      enabled: true
      probability: 0.5
      limit: 25  # degrees
    
  # Photometric transformations (applied only to images)
  photometric:
    invert_image:
      enabled: true
      probability: 0.5
    random_snow:
      enabled: true
      probability: 0.5
      snow_point_lower: 0.1
      snow_point_upper: 0.3
      brightness_coeff: 2.5
    clahe:
      enabled: true
      probability: 0.5
      clip_limit: 4.0
      tile_grid_size: [8, 8]
    blur:
      enabled: true
      probability: 0.5
      blur_limit: 7
    coarse_dropout:
      enabled: true
      probability: 0.5
      num_holes_range: [1, 8]
      hole_height_range: [8, 32]
      hole_width_range: [8, 32]
      fill: 0
    downscale:
      enabled: true
      probability: 0.5
      scale_range: [0.5, 0.9]
      interpolation_upscale: 0  # cv2.INTER_NEAREST
      interpolation_downscale: 0  # cv2.INTER_NEAREST
    equalize:
      enabled: true
      probability: 0.5
      mode: "cv"
      by_channels: false

# Training configuration - Optimized for A100 80GB GPU
training:
  batch_size: 32              # A100 80GB: 32, GTX 1650 Ti 4GB: 1
  gradient_accumulation_steps: 2   # A100 80GB: 2, GTX 1650 Ti 4GB: 32 (to simulate batch_size=32)
  num_workers: 16             # A100 80GB: 16, GTX 1650 Ti 4GB: 2
  shuffle: true
  pin_memory: true
  drop_last: false
  learning_rate: 0.001        # A100 80GB: 0.001, GTX 1650 Ti 4GB: 0.0005 (lower for stability)
  patience: 15
  num_epochs: 100             # A100 80GB: 100, GTX 1650 Ti 4GB: 50

# Model configuration - Optimized for A100 80GB GPU
model:
  input_channels: 1
  output_channels: 3
  base_channels: 64           # A100 80GB: 64, GTX 1650 Ti 4GB: 32
  depth: 5                    # A100 80GB: 5, GTX 1650 Ti 4GB: 4

# Memory and Performance optimization
optimization:
  # Mixed precision training (A100 supports it very well)
  mixed_precision: true       # A100 80GB: true, GTX 1650 Ti 4GB: false (not supported)
  
  # Gradient clipping for stability
  gradient_clip_norm: 1.0
  
  # Memory optimization
  memory_efficient: false     # A100 80GB: false, GTX 1650 Ti 4GB: true
  
  # Compilation optimization (PyTorch 2.0+)
  compile_model: true         # A100 80GB: true, GTX 1650 Ti 4GB: false

# Paths
paths:
  data_dir: "data"
  logs_dir: "logs"
  models_dir: "models"
  results_dir: "results"

# GTX 1650 Ti 4GB Recommended Settings:
# Uncomment and use these settings for GTX 1650 Ti 4GB GPU
# training:
#   batch_size: 1
#   gradient_accumulation_steps: 32
#   num_workers: 2
#   learning_rate: 0.0005
#   num_epochs: 50
# 
# model:
#   base_channels: 32
#   depth: 4
# 
# optimization:
#   mixed_precision: false
#   memory_efficient: true
#   compile_model: false
