{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ba125",
   "metadata": {},
   "source": [
    "# OCT B-scan Annotation Visualization\n",
    "\n",
    "This notebook provides tools to visualize OCT B-scan images with layer annotations from our Hybrid Attention U-Net model. It includes functions to plot ground truth and predicted ILM/BM layer boundaries with comprehensive visualization capabilities.\n",
    "\n",
    "## Model Information\n",
    "- **Model**: Hybrid Attention U-Net for OCT Layer Segmentation\n",
    "- **Training Run**: runs/hybrid_unet_20250917_214714\n",
    "- **Architecture**: Attention-based U-Net with edge and spatial attention mechanisms\n",
    "- **Classes**: 3-class segmentation (Above ILM, ILM-BM, Below BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62886a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "✓ Matplotlib configured for OCT visualization\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import h5py\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Add source directory to path for model imports\n",
    "sys.path.append('scr')\n",
    "\n",
    "# Set matplotlib parameters for better visualization\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(\"✓ Matplotlib configured for OCT visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61109f78",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HybridAttentionUNet.__init__() got an unexpected keyword argument 'input_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m model_loader = ModelLoader(run_path)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Load model and results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m model = \u001b[43mmodel_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m training_results = model_loader.load_training_results()\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Model loaded successfully from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mModelLoader.load_model\u001b[39m\u001b[34m(self, model_filename)\u001b[39m\n\u001b[32m     25\u001b[39m base_channels = model_config.get(\u001b[33m'\u001b[39m\u001b[33mbase_channels\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m32\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create model with correct architecture\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mHybridAttentionUNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_channels\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Load model weights\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: HybridAttentionUNet.__init__() got an unexpected keyword argument 'input_channels'"
     ]
    }
   ],
   "source": [
    "# Load Model and Configuration\n",
    "from model import HybridAttentionUNet\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"Helper class to load trained model and configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, run_path: str):\n",
    "        self.run_path = run_path\n",
    "        self.model = None\n",
    "        self.config = None\n",
    "        self.training_results = None\n",
    "        \n",
    "    def load_model(self, model_filename: str = \"best_model.pth\"):\n",
    "        \"\"\"Load the trained model.\"\"\"\n",
    "        model_path = os.path.join(self.run_path, model_filename)\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # Get model configuration from checkpoint\n",
    "        model_config = checkpoint.get('config', {}).get('model', {})\n",
    "        base_channels = model_config.get('base_channels', 32)\n",
    "        \n",
    "        # Create model with correct architecture\n",
    "        self.model = HybridAttentionUNet(\n",
    "            input_channels=1,\n",
    "            output_channels=3,\n",
    "            base_channels=base_channels\n",
    "        )\n",
    "        \n",
    "        # Load model weights\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"✓ Model loaded from: {model_path}\")\n",
    "        print(f\"✓ Model architecture: base_channels={base_channels}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def load_training_results(self):\n",
    "        \"\"\"Load training results and metadata.\"\"\"\n",
    "        results_files = [\n",
    "            \"training_results.json\",\n",
    "            \"training_summary.json\", \n",
    "            \"final_inference_results.json\"\n",
    "        ]\n",
    "        \n",
    "        self.training_results = {}\n",
    "        for filename in results_files:\n",
    "            filepath = os.path.join(self.run_path, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                with open(filepath, 'r') as f:\n",
    "                    self.training_results[filename.replace('.json', '')] = json.load(f)\n",
    "                print(f\"✓ Loaded: {filename}\")\n",
    "        \n",
    "        return self.training_results\n",
    "\n",
    "# Initialize model loader with our training run\n",
    "run_path = \"runs/hybrid_unet_20250917_214714\"\n",
    "model_loader = ModelLoader(run_path)\n",
    "\n",
    "# Load model and results\n",
    "model = model_loader.load_model()\n",
    "training_results = model_loader.load_training_results()\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully from: {run_path}\")\n",
    "print(f\"✓ Training results loaded: {list(training_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Annotation Plotting Function\n",
    "\n",
    "def mask_to_coordinates(mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert segmentation mask to ILM and BM coordinates.\n",
    "    \n",
    "    Args:\n",
    "        mask: Segmentation mask of shape (H, W) with classes 0, 1, 2\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (ilm_coords, bm_coords) each of shape (W,)\n",
    "    \"\"\"\n",
    "    height, width = mask.shape\n",
    "    ilm_coords = np.full(width, np.nan)\n",
    "    bm_coords = np.full(width, np.nan)\n",
    "    \n",
    "    for x in range(width):\n",
    "        column = mask[:, x]\n",
    "        \n",
    "        # Find ILM boundary (transition from class 0 to class 1)\n",
    "        class_1_pixels = np.where(column == 1)[0]\n",
    "        if len(class_1_pixels) > 0:\n",
    "            ilm_coords[x] = class_1_pixels[0]\n",
    "        \n",
    "        # Find BM boundary (transition from class 1 to class 2)\n",
    "        class_2_pixels = np.where(column == 2)[0]\n",
    "        if len(class_2_pixels) > 0:\n",
    "            bm_coords[x] = class_2_pixels[0]\n",
    "    \n",
    "    return ilm_coords, bm_coords\n",
    "\n",
    "def plot_bscan_annotations(\n",
    "    image: np.ndarray,\n",
    "    gt_ilm: Optional[np.ndarray] = None,\n",
    "    gt_bm: Optional[np.ndarray] = None,\n",
    "    pred_ilm: Optional[np.ndarray] = None,\n",
    "    pred_bm: Optional[np.ndarray] = None,\n",
    "    pred_mask: Optional[np.ndarray] = None,\n",
    "    title: str = \"OCT B-scan with Layer Annotations\",\n",
    "    show_mask_overlay: bool = True,\n",
    "    alpha_mask: float = 0.3,\n",
    "    figsize: Tuple[int, int] = (15, 8)\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot B-scan image with layer annotations.\n",
    "    \n",
    "    Args:\n",
    "        image: B-scan image of shape (H, W)\n",
    "        gt_ilm: Ground truth ILM coordinates (W,)\n",
    "        gt_bm: Ground truth BM coordinates (W,)\n",
    "        pred_ilm: Predicted ILM coordinates (W,)\n",
    "        pred_bm: Predicted BM coordinates (W,)\n",
    "        pred_mask: Predicted segmentation mask (H, W)\n",
    "        title: Plot title\n",
    "        show_mask_overlay: Whether to show segmentation mask overlay\n",
    "        alpha_mask: Transparency of mask overlay\n",
    "        figsize: Figure size\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib Figure object\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    # Display B-scan image\n",
    "    ax.imshow(image, cmap='gray', aspect='auto')\n",
    "    \n",
    "    # Create x-coordinates for plotting\n",
    "    x_coords = np.arange(image.shape[1])\n",
    "    \n",
    "    # Plot ground truth annotations\n",
    "    if gt_ilm is not None:\n",
    "        valid_gt_ilm = ~np.isnan(gt_ilm)\n",
    "        if np.any(valid_gt_ilm):\n",
    "            ax.plot(x_coords[valid_gt_ilm], gt_ilm[valid_gt_ilm], \n",
    "                   'g-', linewidth=2, label='GT ILM', alpha=0.8)\n",
    "    \n",
    "    if gt_bm is not None:\n",
    "        valid_gt_bm = ~np.isnan(gt_bm)\n",
    "        if np.any(valid_gt_bm):\n",
    "            ax.plot(x_coords[valid_gt_bm], gt_bm[valid_gt_bm], \n",
    "                   'b-', linewidth=2, label='GT BM', alpha=0.8)\n",
    "    \n",
    "    # Plot predicted annotations\n",
    "    if pred_ilm is not None:\n",
    "        valid_pred_ilm = ~np.isnan(pred_ilm)\n",
    "        if np.any(valid_pred_ilm):\n",
    "            ax.plot(x_coords[valid_pred_ilm], pred_ilm[valid_pred_ilm], \n",
    "                   'r--', linewidth=2, label='Pred ILM', alpha=0.8)\n",
    "    \n",
    "    if pred_bm is not None:\n",
    "        valid_pred_bm = ~np.isnan(pred_bm)\n",
    "        if np.any(valid_pred_bm):\n",
    "            ax.plot(x_coords[valid_pred_bm], pred_bm[valid_pred_bm], \n",
    "                   'orange', linestyle='--', linewidth=2, label='Pred BM', alpha=0.8)\n",
    "    \n",
    "    # Show segmentation mask overlay\n",
    "    if show_mask_overlay and pred_mask is not None:\n",
    "        # Create colored mask overlay\n",
    "        colors = ['black', 'red', 'blue']  # Classes 0, 1, 2\n",
    "        cmap = ListedColormap(colors)\n",
    "        \n",
    "        # Create mask for overlay (only show non-background classes)\n",
    "        overlay_mask = np.ma.masked_where(pred_mask == 0, pred_mask)\n",
    "        ax.imshow(overlay_mask, cmap=cmap, alpha=alpha_mask, aspect='auto', vmin=0, vmax=2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('A-scan Index (Lateral Position)')\n",
    "    ax.set_ylabel('Pixel Depth (Axial Position)')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Invert y-axis to match OCT convention (depth increases downward)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_bscan_comparison_grid(\n",
    "    images: List[np.ndarray],\n",
    "    gt_ilm_list: List[np.ndarray],\n",
    "    gt_bm_list: List[np.ndarray],\n",
    "    pred_ilm_list: List[np.ndarray],\n",
    "    pred_bm_list: List[np.ndarray],\n",
    "    titles: Optional[List[str]] = None,\n",
    "    figsize: Tuple[int, int] = (20, 12),\n",
    "    cols: int = 2\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot multiple B-scans in a grid for comparison.\n",
    "    \n",
    "    Args:\n",
    "        images: List of B-scan images\n",
    "        gt_ilm_list: List of ground truth ILM coordinates\n",
    "        gt_bm_list: List of ground truth BM coordinates\n",
    "        pred_ilm_list: List of predicted ILM coordinates\n",
    "        pred_bm_list: List of predicted BM coordinates\n",
    "        titles: List of subplot titles\n",
    "        figsize: Figure size\n",
    "        cols: Number of columns in grid\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib Figure object\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    rows = (n_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        row, col = divmod(i, cols)\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Plot B-scan\n",
    "        ax.imshow(images[i], cmap='gray', aspect='auto')\n",
    "        \n",
    "        # Create x-coordinates\n",
    "        x_coords = np.arange(images[i].shape[1])\n",
    "        \n",
    "        # Plot annotations\n",
    "        if gt_ilm_list[i] is not None:\n",
    "            valid_gt_ilm = ~np.isnan(gt_ilm_list[i])\n",
    "            if np.any(valid_gt_ilm):\n",
    "                ax.plot(x_coords[valid_gt_ilm], gt_ilm_list[i][valid_gt_ilm], \n",
    "                       'g-', linewidth=2, label='GT ILM', alpha=0.8)\n",
    "        \n",
    "        if gt_bm_list[i] is not None:\n",
    "            valid_gt_bm = ~np.isnan(gt_bm_list[i])\n",
    "            if np.any(valid_gt_bm):\n",
    "                ax.plot(x_coords[valid_gt_bm], gt_bm_list[i][valid_gt_bm], \n",
    "                       'b-', linewidth=2, label='GT BM', alpha=0.8)\n",
    "        \n",
    "        if pred_ilm_list[i] is not None:\n",
    "            valid_pred_ilm = ~np.isnan(pred_ilm_list[i])\n",
    "            if np.any(valid_pred_ilm):\n",
    "                ax.plot(x_coords[valid_pred_ilm], pred_ilm_list[i][valid_pred_ilm], \n",
    "                       'r--', linewidth=2, label='Pred ILM', alpha=0.8)\n",
    "        \n",
    "        if pred_bm_list[i] is not None:\n",
    "            valid_pred_bm = ~np.isnan(pred_bm_list[i])\n",
    "            if np.any(valid_pred_bm):\n",
    "                ax.plot(x_coords[valid_pred_bm], pred_bm_list[i][valid_pred_bm], \n",
    "                       'orange', linestyle='--', linewidth=2, label='Pred BM', alpha=0.8)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('A-scan Index')\n",
    "        ax.set_ylabel('Pixel Depth')\n",
    "        if titles and i < len(titles):\n",
    "            ax.set_title(titles[i])\n",
    "        else:\n",
    "            ax.set_title(f'B-scan {i+1}')\n",
    "        \n",
    "        if i == 0:  # Only show legend on first subplot\n",
    "            ax.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_images, rows * cols):\n",
    "        row, col = divmod(i, cols)\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Annotation plotting functions defined successfully\")\n",
    "print(\"✓ Available functions:\")\n",
    "print(\"  - plot_bscan_annotations(): Single B-scan visualization\")\n",
    "print(\"  - plot_bscan_comparison_grid(): Multi B-scan comparison\")\n",
    "print(\"  - mask_to_coordinates(): Convert segmentation to coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load B-scan Data and Inference Results\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Helper class to load B-scan data and generate predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str = \"data/Nemours_Jing_0805.h5\"):\n",
    "        self.data_path = data_path\n",
    "        self.images = None\n",
    "        self.ilm_coords = None\n",
    "        self.bm_coords = None\n",
    "        \n",
    "    def load_original_data(self):\n",
    "        \"\"\"Load original B-scan images and annotations.\"\"\"\n",
    "        with h5py.File(self.data_path, 'r') as f:\n",
    "            self.images = f['images'][:]  # Shape: (N, 496, 768)\n",
    "            self.ilm_coords = f['layers']['ILM'][:]  # Shape: (N, 768)\n",
    "            self.bm_coords = f['layers']['BM'][:]    # Shape: (N, 768)\n",
    "        \n",
    "        print(f\"✓ Loaded original data from: {self.data_path}\")\n",
    "        print(f\"✓ Images shape: {self.images.shape}\")\n",
    "        print(f\"✓ ILM coords shape: {self.ilm_coords.shape}\")\n",
    "        print(f\"✓ BM coords shape: {self.bm_coords.shape}\")\n",
    "        \n",
    "        return self.images, self.ilm_coords, self.bm_coords\n",
    "    \n",
    "    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess image for model inference (resize to 480x768).\"\"\"\n",
    "        # Resize from (496, 768) to (480, 768)\n",
    "        resized = cv2.resize(image, (768, 480), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Apply preprocessing (CLAHE)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        processed = clahe.apply(resized.astype(np.uint8))\n",
    "        \n",
    "        # Normalize\n",
    "        normalized = (processed.astype(np.float32) / 255.0)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def resize_coordinates(self, coords: np.ndarray, original_height: int = 496, target_height: int = 480) -> np.ndarray:\n",
    "        \"\"\"Resize coordinates to match target image size.\"\"\"\n",
    "        scale_factor = target_height / original_height\n",
    "        resized_coords = coords.copy()\n",
    "        \n",
    "        valid_mask = ~np.isnan(resized_coords)\n",
    "        resized_coords[valid_mask] *= scale_factor\n",
    "        \n",
    "        return resized_coords\n",
    "    \n",
    "    def generate_predictions(self, model: torch.nn.Module, indices: Optional[List[int]] = None) -> Dict:\n",
    "        \"\"\"Generate model predictions for specified indices.\"\"\"\n",
    "        if self.images is None:\n",
    "            self.load_original_data()\n",
    "        \n",
    "        if indices is None:\n",
    "            indices = list(range(min(10, len(self.images))))  # First 10 samples by default\n",
    "        \n",
    "        predictions = {\n",
    "            'indices': indices,\n",
    "            'original_images': [],\n",
    "            'processed_images': [],\n",
    "            'gt_ilm': [],\n",
    "            'gt_bm': [],\n",
    "            'pred_masks': [],\n",
    "            'pred_ilm': [],\n",
    "            'pred_bm': []\n",
    "        }\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx in indices:\n",
    "                # Get original data\n",
    "                original_img = self.images[idx]\n",
    "                gt_ilm = self.ilm_coords[idx]\n",
    "                gt_bm = self.bm_coords[idx]\n",
    "                \n",
    "                # Preprocess for model\n",
    "                processed_img = self.preprocess_image(original_img)\n",
    "                \n",
    "                # Resize ground truth coordinates\n",
    "                resized_gt_ilm = self.resize_coordinates(gt_ilm)\n",
    "                resized_gt_bm = self.resize_coordinates(gt_bm)\n",
    "                \n",
    "                # Convert to tensor and add batch dimension\n",
    "                input_tensor = torch.from_numpy(processed_img).unsqueeze(0).unsqueeze(0)  # (1, 1, 480, 768)\n",
    "                \n",
    "                # Generate prediction\n",
    "                output = model(input_tensor)\n",
    "                pred_logits = output.squeeze(0)  # (3, 480, 768)\n",
    "                pred_mask = torch.argmax(pred_logits, dim=0).numpy()  # (480, 768)\n",
    "                \n",
    "                # Convert mask to coordinates\n",
    "                pred_ilm, pred_bm = mask_to_coordinates(pred_mask)\n",
    "                \n",
    "                # Store results\n",
    "                predictions['original_images'].append(original_img)\n",
    "                predictions['processed_images'].append(processed_img)\n",
    "                predictions['gt_ilm'].append(resized_gt_ilm)\n",
    "                predictions['gt_bm'].append(resized_gt_bm)\n",
    "                predictions['pred_masks'].append(pred_mask)\n",
    "                predictions['pred_ilm'].append(pred_ilm)\n",
    "                predictions['pred_bm'].append(pred_bm)\n",
    "        \n",
    "        print(f\"✓ Generated predictions for {len(indices)} B-scans\")\n",
    "        return predictions\n",
    "\n",
    "# Initialize data loader and load sample data\n",
    "data_loader = DataLoader()\n",
    "original_images, original_ilm, original_bm = data_loader.load_original_data()\n",
    "\n",
    "# Generate predictions for first 5 B-scans\n",
    "sample_indices = [0, 100, 200, 300, 400]  # Sample different B-scans\n",
    "predictions = data_loader.generate_predictions(model, sample_indices)\n",
    "\n",
    "print(f\"\\\\n✓ Sample data loaded and predictions generated\")\n",
    "print(f\"✓ Available B-scan indices: {predictions['indices']}\")\n",
    "print(f\"✓ Processed image shape: {predictions['processed_images'][0].shape}\")\n",
    "print(f\"✓ Prediction mask shape: {predictions['pred_masks'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize B-scan with Annotations\n",
    "\n",
    "# Display a single B-scan with detailed annotations\n",
    "sample_idx = 0  # First sample\n",
    "sample_image = predictions['processed_images'][sample_idx]\n",
    "sample_gt_ilm = predictions['gt_ilm'][sample_idx]\n",
    "sample_gt_bm = predictions['gt_bm'][sample_idx]\n",
    "sample_pred_ilm = predictions['pred_ilm'][sample_idx]\n",
    "sample_pred_bm = predictions['pred_bm'][sample_idx]\n",
    "sample_pred_mask = predictions['pred_masks'][sample_idx]\n",
    "\n",
    "# Create detailed visualization\n",
    "fig = plot_bscan_annotations(\n",
    "    image=sample_image,\n",
    "    gt_ilm=sample_gt_ilm,\n",
    "    gt_bm=sample_gt_bm,\n",
    "    pred_ilm=sample_pred_ilm,\n",
    "    pred_bm=sample_pred_bm,\n",
    "    pred_mask=sample_pred_mask,\n",
    "    title=f\"OCT B-scan #{predictions['indices'][sample_idx]} - Ground Truth vs Predictions\",\n",
    "    show_mask_overlay=True,\n",
    "    alpha_mask=0.3\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display prediction statistics\n",
    "print(f\"\\\\n📊 Prediction Statistics for B-scan #{predictions['indices'][sample_idx]}:\")\n",
    "print(f\"   Image size: {sample_image.shape}\")\n",
    "print(f\"   GT ILM points: {np.sum(~np.isnan(sample_gt_ilm))}/{len(sample_gt_ilm)}\")\n",
    "print(f\"   GT BM points: {np.sum(~np.isnan(sample_gt_bm))}/{len(sample_gt_bm)}\")\n",
    "print(f\"   Pred ILM points: {np.sum(~np.isnan(sample_pred_ilm))}/{len(sample_pred_ilm)}\")\n",
    "print(f\"   Pred BM points: {np.sum(~np.isnan(sample_pred_bm))}/{len(sample_pred_bm)}\")\n",
    "\n",
    "# Calculate prediction errors where both GT and pred are valid\n",
    "valid_ilm = ~np.isnan(sample_gt_ilm) & ~np.isnan(sample_pred_ilm)\n",
    "valid_bm = ~np.isnan(sample_gt_bm) & ~np.isnan(sample_pred_bm)\n",
    "\n",
    "if np.any(valid_ilm):\n",
    "    ilm_mae = np.mean(np.abs(sample_gt_ilm[valid_ilm] - sample_pred_ilm[valid_ilm]))\n",
    "    print(f\"   ILM MAE: {ilm_mae:.2f} pixels\")\n",
    "\n",
    "if np.any(valid_bm):\n",
    "    bm_mae = np.mean(np.abs(sample_gt_bm[valid_bm] - sample_pred_bm[valid_bm]))\n",
    "    print(f\"   BM MAE: {bm_mae:.2f} pixels\")\n",
    "\n",
    "# Show class distribution in prediction mask\n",
    "unique, counts = np.unique(sample_pred_mask, return_counts=True)\n",
    "total_pixels = sample_pred_mask.size\n",
    "print(f\"\\\\n🎯 Segmentation Class Distribution:\")\n",
    "class_names = [\"Above ILM\", \"ILM-BM\", \"Below BM\"]\n",
    "for class_id, count in zip(unique, counts):\n",
    "    percentage = (count / total_pixels) * 100\n",
    "    print(f\"   Class {class_id} ({class_names[class_id]}): {count:,} pixels ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Multiple B-scans\n",
    "\n",
    "# Create comparison grid for all loaded samples\n",
    "fig = plot_bscan_comparison_grid(\n",
    "    images=predictions['processed_images'],\n",
    "    gt_ilm_list=predictions['gt_ilm'],\n",
    "    gt_bm_list=predictions['gt_bm'],\n",
    "    pred_ilm_list=predictions['pred_ilm'],\n",
    "    pred_bm_list=predictions['pred_bm'],\n",
    "    titles=[f\"B-scan #{idx}\" for idx in predictions['indices']],\n",
    "    figsize=(20, 15),\n",
    "    cols=3\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Analyze prediction quality across all samples\n",
    "print(\"\\\\n📈 Prediction Quality Analysis Across All Samples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_ilm_errors = []\n",
    "all_bm_errors = []\n",
    "all_valid_ilm_counts = []\n",
    "all_valid_bm_counts = []\n",
    "\n",
    "for i, idx in enumerate(predictions['indices']):\n",
    "    gt_ilm = predictions['gt_ilm'][i]\n",
    "    gt_bm = predictions['gt_bm'][i]\n",
    "    pred_ilm = predictions['pred_ilm'][i]\n",
    "    pred_bm = predictions['pred_bm'][i]\n",
    "    \n",
    "    # Calculate valid points\n",
    "    valid_ilm = ~np.isnan(gt_ilm) & ~np.isnan(pred_ilm)\n",
    "    valid_bm = ~np.isnan(gt_bm) & ~np.isnan(pred_bm)\n",
    "    \n",
    "    valid_ilm_count = np.sum(valid_ilm)\n",
    "    valid_bm_count = np.sum(valid_bm)\n",
    "    \n",
    "    all_valid_ilm_counts.append(valid_ilm_count)\n",
    "    all_valid_bm_counts.append(valid_bm_count)\n",
    "    \n",
    "    # Calculate errors\n",
    "    if valid_ilm_count > 0:\n",
    "        ilm_mae = np.mean(np.abs(gt_ilm[valid_ilm] - pred_ilm[valid_ilm]))\n",
    "        all_ilm_errors.append(ilm_mae)\n",
    "        ilm_status = f\"MAE: {ilm_mae:.2f}px\"\n",
    "    else:\n",
    "        ilm_status = \"No valid points\"\n",
    "    \n",
    "    if valid_bm_count > 0:\n",
    "        bm_mae = np.mean(np.abs(gt_bm[valid_bm] - pred_bm[valid_bm]))\n",
    "        all_bm_errors.append(bm_mae)\n",
    "        bm_status = f\"MAE: {bm_mae:.2f}px\"\n",
    "    else:\n",
    "        bm_status = \"No valid points\"\n",
    "    \n",
    "    print(f\"B-scan #{idx:3d}: ILM {ilm_status:15s} | BM {bm_status:15s}\")\n",
    "\n",
    "# Overall statistics\n",
    "if all_ilm_errors:\n",
    "    print(f\"\\\\n🎯 Overall ILM Performance:\")\n",
    "    print(f\"   Mean MAE: {np.mean(all_ilm_errors):.2f} ± {np.std(all_ilm_errors):.2f} pixels\")\n",
    "    print(f\"   Best MAE: {np.min(all_ilm_errors):.2f} pixels\")\n",
    "    print(f\"   Worst MAE: {np.max(all_ilm_errors):.2f} pixels\")\n",
    "\n",
    "if all_bm_errors:\n",
    "    print(f\"\\\\n🎯 Overall BM Performance:\")\n",
    "    print(f\"   Mean MAE: {np.mean(all_bm_errors):.2f} ± {np.std(all_bm_errors):.2f} pixels\")\n",
    "    print(f\"   Best MAE: {np.min(all_bm_errors):.2f} pixels\")\n",
    "    print(f\"   Worst MAE: {np.max(all_bm_errors):.2f} pixels\")\n",
    "\n",
    "print(f\"\\\\n📊 Coverage Statistics:\")\n",
    "print(f\"   ILM valid points: {np.mean(all_valid_ilm_counts):.1f} ± {np.std(all_valid_ilm_counts):.1f} / 768\")\n",
    "print(f\"   BM valid points: {np.mean(all_valid_bm_counts):.1f} ± {np.std(all_valid_bm_counts):.1f} / 768\")\n",
    "\n",
    "# Display training performance for comparison\n",
    "if 'final_inference_results' in training_results:\n",
    "    final_results = training_results['final_inference_results']\n",
    "    print(f\"\\\\n🏆 Model Training Performance (from final_inference_results.json):\")\n",
    "    print(f\"   Test Dice Score: {final_results['segmentation_metrics']['dice']:.4f}\")\n",
    "    print(f\"   Test ILM MAE: {final_results['regression_metrics']['ilm_mae']:.2f} pixels\")\n",
    "    print(f\"   Test BM MAE: {final_results['regression_metrics']['bm_mae']:.2f} pixels\")\n",
    "    print(f\"   Overall MAE: {final_results['regression_metrics']['overall_mae']:.2f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac6b06",
   "metadata": {},
   "source": [
    "## Interactive Visualization Functions\n",
    "\n",
    "The notebook above provides comprehensive tools for visualizing OCT B-scan annotations. Here's a summary of what you can do:\n",
    "\n",
    "### Key Functions Available:\n",
    "\n",
    "1. **`plot_bscan_annotations()`** - Main visualization function\n",
    "   - Displays B-scan image with ground truth and predicted layer boundaries\n",
    "   - Supports segmentation mask overlay\n",
    "   - Customizable colors, transparency, and styling\n",
    "\n",
    "2. **`plot_bscan_comparison_grid()`** - Multi-sample comparison\n",
    "   - Grid layout for comparing multiple B-scans\n",
    "   - Side-by-side ground truth vs predictions\n",
    "   - Useful for batch analysis\n",
    "\n",
    "3. **`mask_to_coordinates()`** - Utility function\n",
    "   - Converts segmentation masks to coordinate boundaries\n",
    "   - Handles ILM and BM layer extraction\n",
    "\n",
    "### Visualization Features:\n",
    "\n",
    "- **Ground Truth Layers**: Green (ILM) and Blue (BM) solid lines\n",
    "- **Predicted Layers**: Red (ILM) and Orange (BM) dashed lines  \n",
    "- **Segmentation Overlay**: Colored regions showing tissue classification\n",
    "- **Error Analysis**: Pixel-level accuracy metrics\n",
    "- **Coverage Statistics**: Valid annotation point counts\n",
    "\n",
    "### Model Performance Metrics:\n",
    "\n",
    "The notebook automatically displays:\n",
    "- Mean Absolute Error (MAE) for each layer\n",
    "- Valid annotation coverage\n",
    "- Comparison with training results\n",
    "- Class distribution in segmentation masks\n",
    "\n",
    "### Usage Example:\n",
    "\n",
    "```python\n",
    "# Plot single B-scan with annotations\n",
    "fig = plot_bscan_annotations(\n",
    "    image=sample_image,\n",
    "    gt_ilm=ground_truth_ilm,\n",
    "    gt_bm=ground_truth_bm, \n",
    "    pred_ilm=predicted_ilm,\n",
    "    pred_bm=predicted_bm,\n",
    "    show_mask_overlay=True\n",
    ")\n",
    "```\n",
    "\n",
    "This provides a complete framework for analyzing your Hybrid Attention U-Net model's performance on OCT layer segmentation tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
